# Section 00 - Getting ready
1. Make sure you have already installed and loaded the following libraries:
```{r}
library(ggplot2)
library(data.table)
library(magrittr)
library(tidyr)
library(dplyr)
library(patchwork)
```

2. Load yeast data and required packages using the following code:
```{r}
genotype <-fread("extdata/eqtl/genotype.txt")
growth_rate <-fread("extdata/eqtl/growth.txt")
marker <-fread("extdata/eqtl/marker.txt")
setnames(marker, "id", "marker")
genotype <- genotype %>%
melt(id.vars = "strain", variable.name = "marker", value.name = "genotype")
```

#Section 01 - Quantile-Quantile plots
We will simulate some data from different distributions and will compare their quantile-quantile plots.

1. We will use a standard normal (μ = 0 and σ2 = 1) distribution as a reference set. Please simulate 100
draws from a standard normal distribution. Add them as a column to a data table and plot a histogram of
these values. Next, use ggplot to create a QQ-plot comparing the expected against the observed quantiles.
Use geom_abline to draw a line on the QQ-plot where the data should be if the observed and expected
quantiles match exactly. (Do not mind warnings, if any).
Hint 1: Set the x and y limits to [-6,6] where appropriate.
Hint 2: A small reminder of the R functions to simulate a normal distribution:
```{r}
set.seed(10)
# rnorm allows you to draw random values from a normal distribution
rnorm(10) # 10 random draws
# pnorm gives the cumulative probability from a normal
# i.e. pnorm(x) = p(X < x) where X is the random variable
pnorm(0)
# qnorm returns the quantiles of a normal distribution
# it is the inverse of pnorm
# i.e. given a probability p,
# it finds x so that pnorm(x) = p
qnorm(0.5)
# qnorm can be used to find different types of quantiles
qnorm(seq(0.25,0.75,0.25)) # quartiles of the normal
qnorm(seq(0.1,0.9,0.1)) # deciles of the normal
```

```{r}
set.seed(42)

values <- data.table(
  norm = rnorm(100)
)
values

ggplot(values, aes(norm)) + 
  geom_histogram()

quantiles <- c(.25, .5, .75)
quantiles <- seq(.1, .9, .1)

ggplot(data.table(
  y = quantile(values$norm, quantiles),
  x = sapply(quantiles, qnorm)), 
  aes(x, y)) +
  geom_point() +
  geom_abline(intercept=0, slope=1) +
  xlim(-6, 6) 

ggplot(values, aes(sample=norm)) + 
  geom_qq(distribution = stats::qnorm) +
  geom_abline(intercept=0, slope=1) +
  xlim(-6, 6) 
```

2. Now add a normal distribution with μ = 4 to your data.table and plot the Q-Q plot. How did it change?
```{r}
values$norm_shift <- rnorm(100, mean = 4)

ggplot(data.table(
  y = quantile(values$norm_shift, quantiles),
  x = sapply(quantiles, qnorm)), 
  aes(x, y)) +
  geom_point() +
  geom_abline(intercept=0, slope=1) +
  xlim(-6, 6)

ggplot(values, aes(sample=norm_shift)) + 
  geom_qq(distribution = stats::qnorm) +
  geom_abline(intercept=0, slope=1) +
  xlim(-6, 6) 
```

3. How would you tweak the distribution so that you get the following Q-Q plot?
```{r}
# larger standard deviation
values$norm_squash <- rnorm(100, sd = 4)

ggplot(data.table(
  y = quantile(values$norm_squash, quantiles),
  x = sapply(quantiles, qnorm)), 
  aes(x, y)) +
  geom_point() +
  geom_abline(intercept=0, slope=1) +
  xlim(-6, 6)

ggplot(values, aes(sample=norm_squash)) + 
  geom_qq(distribution = stats::qnorm) +
  geom_abline(intercept=0, slope=1) +
  xlim(-6, 6) 
```



# Section 02 - QTL mapping of growth
For the next questions, we will use the yeast dataset.

1. Test for markers associated with growth.
Report genetic markers significantly associating with growth rate in maltose. We would like the expected
ratio of false discoveries among the reported significant markers to not exceed 5%.
To do so, run a Wilcoxon test for growth rate versus the genotype at each of the 1,000 markers. Remember
that we tested this relationship for one specific marker last time. Plot a histogram and a Q-Q plot of the
obtained P -values. Which ones would you consider significant and why? Do we need to correct for multiple
testing?
Hint: plot P -values in -log10 scale. Use ppoints(pval, a=0) to generate the expected quantiles according
to the uniform distribution, i.e. 1/(n + 1), . . . , n/(n + 1). Note that this is similar to using geom_qq and then
log-scaling the axes, except that we want to log-scale and also reverse.
```{r}
# we first need to merge the genotype and the growth rate tables.
genotype_growth <- merge(genotype, growth_rate, by = 'strain')
# We can then execute the test for each marker inside a data.table function and extract the p-values.
test_res <- genotype_growth[, .(pval=wilcox.test(YPMalt ~ genotype)$p.value), by='marker']
ggplot(test_res, aes(pval)) + geom_histogram(boundary = TRUE, bins=50)

ppoints(test_res$pval, a=0)

ggplot(test_res[order(pval)], aes(-log10(ppoints(pval, a=0)), -log10(pval))) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1)
```


2. Plot the P -values against genomic position. Do you see positions that are associated with growth? The
genomic position is defined by the chromosome the marker is on and the marker’s position within that
chromosome.
Hint: plot P -values in -log10 scale. Use the start column from the marker table as position. Additionally,
facet on the chrom column (because the start column indicates the genomic position of the marker within
its particular chromosome).
```{r}
ggplot(merge(test_res, marker, by = "marker"), aes(start, -log10(pval), color=chrom)) +
  geom_point() +
  facet_wrap(~chrom, scales = "free_x")
```

3. How many markers significantly associate with growth before and after correcting for multiple testing?
```{r}
test_res[, padj := p.adjust(pval, method="BH")]

test_res[pval < 0.05, .N]
test_res[padj < 0.05, .N]
```



# Section 03 - Let’s do many tests
1. Consider the dataset mtcars. Find all pairs of quantitative variables that show significant association (at
the 5% level) with each other. Do not make any assumptions of distribution.
Hint: for a solution that does not use for-loops, use combn and apply.
```{r}
data(mtcars)

combinations <- combn(c("mpg","disp","hp","drat","wt","qsec"), m=2)

calc_cor_pval <- function(x) {
  v1 <- t(combinations)[x,1]
  v2 <- t(combinations)[x,2]
  
  cor.test(mtcars[, v1], mtcars[, v2], method = "spearman")$p.value
}

pvalues <- sapply(1:ncol(combinations), calc_cor_pval)

comb_pvalues <- data.table(
  V1 = t(combinations)[,1],
  V2 = t(combinations)[,2],
  pvalue = pvalues
)

comb_pvalues
comb_pvalues[pvalue<0.05]
```

2. Now ensure that, on average, less than 5% of the significant associations you find are false positives.
```{r}
comb_pvalues[, BH := p.adjust(pvalue, method = "BH")]
comb_pvalues[BH<0.05]
```

3. Now ensure that the probability of having 1 or more false positives is less than 5%
```{r}
comb_pvalues[, bonferroni := p.adjust(pvalue, method = "bonferroni")]
comb_pvalues[bonferroni<0.05]
```



# Section 04 - P-values and FDR
Here we will use simulations to investigate the effect of sample size and of the proportion of true and false
null hypotheses when performing multiple testing. We will do it for the problem of two-sample comparison
with equal sizes.
We are interested in comparing the observations of two samples: x1, ..., xn and y1, ..., yn. Specifically, we ask
whether the expectations differ using a two-sample Student t-test.

1. Simulate data under the null hypothesis H0 : μx = μy = 0.
We simulate N = 10, 000 times two samples x1, ..., xn and y1, ..., yn where X and Y follow the standard
normal distribution. We use sample size n = 50 for each group. For each simulated dataset, we compute the
two-sided P -value of a t-test. We assume unequal variance as by default in the R function t.test().
You can use the following functions for this exercise:
```{r}
simulate_norm_groups <- function(sample_size=50, N_experiments=10000, mu_x=0, mu_y=0){
  sapply(seq(N_experiments), function(i){
  x <- rnorm(sample_size, mu_x)
  y <- rnorm(sample_size, mu_y)
  t.test(x, y, alternative="two.sided")$p.value
  })
}
plot_pval <- function(pvals, title="p-val distribution"){
  pval_dt <- data.table(pvals=pvals)
  histo <- ggplot(pval_dt, aes(pvals)) + geom_histogram(boundary = TRUE) +
    labs(title = title)
    qq <- ggplot(data = pval_dt, aes(sample = pvals)) +
    geom_qq(distribution = stats::qunif, dparams = list(min = 0, max = 1)) +
    geom_abline(a=0,b=1) +
    ylim(c(0,1)) +
    labs(title = title)
  histo + qq
}
```

```{r}
pvals <- simulate_norm_groups()
```


2. compute the quantiles and plot a histogram and a QQ-plot
If all tests are truly under the null hypothesis, the distribution of the P -values should be uniform by definition.
Please plot the P -values for sample_size = 50 with the provided function. Discuss.
```{r}
quantile(pvals, c(.25,.5,.75))
quantile(pvals, seq(.1, .9, .1))
plot_pval(pvals)
```
Seems very uniform (see qq plot)

3. Correct for multiple testing
Adjust P -values with the different methods seen in the class. Plot the results using the plot function. Do
they behave as expected? Discuss.
```{r}
pvals_table <- data.table(
  pvalue = pvals
)
pvals_table[, bonferroni := p.adjust(pvalue, method="bonferroni")]
pvals_table[, BH := p.adjust(pvalue, method="BH")]
pvals_table

plot_pval(pvals_table$bonferroni, title = "bonferroni")
plot_pval(pvals_table$BH, title = "BH")
```



# Section 05 - sample size and power

1. We will now simulate data under the alternative hypothesis H1 : μx̸ = μy . Specifically, we simulate two
samples x1, ..., xn and y1, ..., yn where X and Y follow the normal distribution with μx = 0 and μy = 0.5
respectively. Do N = 1, 000 experiments and investigate the effect of different sample sizes n (10, 100, 1000)
on the P -value plots. Discuss.
```{r}
pvals_10 <- simulate_norm_groups(sample_size = 10, N_experiments = 1000, mu_x = 0, mu_y = 0.5)
pvals_100 <- simulate_norm_groups(sample_size = 100, N_experiments = 1000, mu_x = 0, mu_y = 0.5)
pvals_1000 <- simulate_norm_groups(sample_size = 1000, N_experiments = 1000, mu_x = 0, mu_y = 0.5)

plot_pval(pvals_10)
plot_pval(pvals_100)
plot_pval(pvals_1000)
```
The more observations you have, the lower the pvalue gets
Small deviations are noticable with enough observations

2. P -values for a mixture of null and alternative.
Provide the same plots as before when considering a dataset of N0 = 10000 data points simulated under H0
(true null) and N1 = 1000 data points simulated under H1 (false null). Discuss. You can also use a -log10
transformation, as in the tutorial, to better visualize the lower end of P -values. The following function will
allow us to plot the -log10 transformed P -values in a QQ-plot:
```{r}
plot_pval_log10 <- function(pvals, title="p val distribution"){
  n <- length(pvals)
  dt <- data.table(
    observed = -log10(sort(pvals)),
    expected = -log10(ppoints(n, a=0))
  )
  ggplot(dt) +
    geom_point(aes(expected, observed)) +
    geom_abline(intercept = 0, slope = 1)
}

pvals_H0 <- simulate_norm_groups(sample_size = 50, N_experiments = 10000)
pvals_H1 <- simulate_norm_groups(sample_size = 50, N_experiments = 1000, mu_x = 0, mu_y = 0.5)

plot_pval(c(pvals_H0, pvals_H1))
plot_pval_log10(c(pvals_H0, pvals_H1))
```
3. Mixture of H0 and H1 adjusted for multiple testing
Adjust the p-values with Benjamini-Hochberg (FDR) in the mixture from the previous question. Make a
contingency table of true positives, true negatives, false positives and false negatives. Try this with different
sample sizes for FDR = 0.05. Discuss.
Do the same thing for the bonferroni correction and compare the results
Hint: You can use the following function for this analysis:
```{r}
error_analysis <- function(method='BH', sample_size=50, cut=0.05){
  pvals <- c(
    simulate_norm_groups(sample_size = sample_size, N_experiments = 10000),
    simulate_norm_groups(sample_size = sample_size, N_experiments = 1000, mu_y=0.5)
  )
  names(pvals) <- rep(c("H0", "H1"), c(10000, 1000))
  pvals_adj <- p.adjust(pvals, method=method)
  table(ifelse(pvals_adj < cut, "significant", "not significant"), names(pvals))
}
```


```{r}
sample_size <- 10
error_analysis(sample_size = sample_size, method="BH")
error_analysis(sample_size = sample_size, method="bonferroni")
```

```{r}
sample_size <- 100
error_analysis(sample_size = sample_size, method="BH")
error_analysis(sample_size = sample_size, method="bonferroni")
```

```{r}
sample_size <- 1000
error_analysis(sample_size = sample_size, method="BH")
error_analysis(sample_size = sample_size, method="bonferroni")
```

bonferroni has significantly less false positives but therefore more false negatives
BH has less false negatives but more false positives

Unless sample size is massive (their tradeoffs are less noticable with more observations)





