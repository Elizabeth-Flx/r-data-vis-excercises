# Section 00 - Getting ready
1. Make sure you have already installed and loaded the following libraries:
```{r}
library(ggplot2)
library(data.table)
library(magrittr)
library(tidyr)
library(ggrepel)
library(plotROC)
```



# Section 01 - Logistic regression on Diabetes dataset

In this section we are considering the dataset pima-indians-diabetes.csv which is originally from the
National Institute of Diabetes and Digestive and Kidney Diseases. A more detailed description of the data
can be obtained from Kaggle: https://www.kaggle.com/uciml/pima-indians-diabetes-database.
Load the dataset with the following lines of code:
```{r}
diabetes_dt <- fread("extdata/pima-indians-diabetes.csv")
diabetes_dt[, Outcome := as.factor(Outcome)]
# Store feature variables that we will need for later
feature_vars <- colnames(diabetes_dt[,-c("Outcome")])
diabetes_dt
```
1. How balanced are the classes of the diabetes dataset?
```{r}
diabetes_dt[, .N, by=Outcome]
```

2. Create an appropriate plot to visualize the relationship between the Outcome variable and the feature
variables Glucose, BloodPressure and Insulin. What do you conclude from your visualization?
```{r}
ggplot(diabetes_dt, aes(Glucose, Outcome)) +
  geom_violin() +
  geom_boxplot(width=.2)

ggplot(diabetes_dt, aes(BloodPressure, Outcome)) +
  geom_violin() +
  geom_boxplot(width=.2)

ggplot(diabetes_dt, aes(Insulin, Outcome)) +
  geom_violin() +
  geom_boxplot(width=.2)
```

3. Fit a logistic regression model for predicting Outcome only based on the feature Glucose. Inspect the
coefficients of the modelâ€™s predictors. According to the model, how much do the odds of getting diabetes
increase upon increasing the blood glucose level by 1 mg/dL?
```{r}
glucose_fit <- glm(Outcome ~ Glucose, data=diabetes_dt, family = "binomial")
glucose_fit

coef(glucose_fit)

exp(coef(glucose_fit)[2])
```
increase in 1 mg/dL associates with an increase of around 4% of the odds of getting diabetes

4. Collect the predictions for the model from above for all samples in the dataset. Store the scores in a new
column of the original dataset. Visualize the distributions of the scores with an appropriate plot. Which type
of distribution would you ideally expect? Hint: Use the predict() function. (normal)
```{r}
diabetes_dt

diabetes_dt[, prediction := predict(glucose_fit, data=diabetes_dt$Glucose)]

ggplot(diabetes_dt, aes(prediction)) +
  geom_histogram()

ggplot(diabetes_dt, aes(prediction, fill=Outcome)) +
  geom_histogram(position = "dodge")
```


5. Now, create a function for computing the confusion matrix based on the predicted scores of a model and
the actual outcome. The function takes as input a threshold, a data table, the name of a scores column and
the name of column with the actual labels. Then, use the implemented function for computing the confusion
matrix of the model for the thresholds -1, 0 and 1. Are there any differences? What is the amount of false
positives for the last cutoff? (11) You can use the following definition of the function:
```{r}
confusion_matrix <- function(dt, score_column, labels_column, threshold){
  
  t_neg <- dt[get(labels_column)==0 & get(score_column) < threshold, .N]
  f_pos <- dt[get(labels_column)==0 & get(score_column) >= threshold, .N]
  f_neg <- dt[get(labels_column)==1 & get(score_column) < threshold, .N]
  t_pos <- dt[get(labels_column)==1 & get(score_column) >= threshold, .N]
  
  
  conf_mat <- data.frame(
    negative_label = c(t_neg,f_pos),
    positive_label = c(f_neg,t_pos)
  ) 
  rownames(conf_mat) <- c("negative_prediction", "positive_prediction")
  conf_mat
}

confusion_matrix(diabetes_dt, "prediction", "Outcome", -1)
confusion_matrix(diabetes_dt, "prediction", "Outcome", 0)
confusion_matrix(diabetes_dt, "prediction", "Outcome", 1)
```

6. Use the implemented function to create a second function for this time computing the TPR and FPR for
a certain threshold of a classification model given the predicted scores of a model and the actual outcome.
What is the TPR and the FPR of the first model for the thresholds -1, 0 and 1? Plot these values in a scatter
plot. Your function should take the same parameters as before and return a data table as follows:
```{r}
tpr_fpr <- function(dt, score_column, labels_column, threshold){
  
  cm <- confusion_matrix(dt, score_column, labels_column, threshold)
  
  tn <- cm["negative_prediction", "negative_label"]
  fn <- cm["negative_prediction", "positive_label"]
  tp <- cm["positive_prediction", "positive_label"]
  fp <- cm["positive_prediction", "negative_label"]
  
  tpr <- tp / (tp+fn)
  fpr <- fp / (tn+fp)
  
  return(data.table(tpr=tpr, fpr=fpr, t=threshold))
}

thresholds <- c(-1, 0, 1)
thresholds <- seq(-5, 5, .01)

points <- lapply(thresholds, function(t) {
  tpr_fpr(diabetes_dt, "prediction", "Outcome", t)
})

rbindlist(points)

ggplot(rbindlist(points), aes(fpr, tpr, label=t)) +
  geom_point() +
  xlim(0,1) +
  ylim(0,1)
```


# Homework

1. Create two further logistic regression models as in section 1.3 for predicting Outcome. For one model, use
only the feature variable BloodPressure for building the model. For the other model, use only the feature
variable Insulin. Which models have a significant feature?
```{r}
g_fit <- glm(Outcome ~ Glucose, data=diabetes_dt, family = "binomial")
b_fit <- glm(Outcome ~ BloodPressure, data=diabetes_dt, family = "binomial")
i_fit <- glm(Outcome ~ Insulin, data=diabetes_dt, family = "binomial")

exp(coef(g_fit)[2]) 
exp(coef(b_fit)[2])
exp(coef(i_fit)[2])
```

2. Collect the predictions of each model for all samples in the dataset. Store the scores of each model in a
separate column of the original dataset. Visualize the distributions of the scores with an appropriate plot.
Which type of distribution would you ideally expect? Hint: Use the predict() function.
```{r}
diabetes_dt[, g_prediction := predict(g_fit, data=Glucose)]
diabetes_dt[, b_prediction := predict(b_fit, data=BloodPressure)]
diabetes_dt[, i_prediction := predict(i_fit, data=Insulin)]

diabetes_dt

ggplot(diabetes_dt, aes(g_prediction, fill=Outcome)) + geom_histogram(position = "dodge")
ggplot(diabetes_dt, aes(b_prediction, fill=Outcome)) + geom_histogram(position = "dodge")
ggplot(diabetes_dt, aes(i_prediction, fill=Outcome)) + geom_histogram(position = "dodge")
```

3. For a systematic comparison of the previously built three models, plot a ROC curve for each model into a
single plot using the function geom_roc from the library plotROC. Add the area under the curve (AUC) to
the plot. Which is the best model according to the AUC?
```{r}
melted <- melt(diabetes_dt, measure.vars = c("g_prediction", "b_prediction", "i_prediction"), variable.name = "predictor", value.name = "response")

ggplot(melted, aes(d=as.numeric(Outcome), m=response, color=predictor)) +
  geom_roc() +
  geom_abline(intercept = 0, slope = 1)




```

4. Now, fit a logistic regression model with all feature variables (stored in feature_vars). Visualize the
distribution of the predicted scores for positive and negative classes. What can you conclude from this
visualization regarding the separation of the two classes by the model? Plot once again the previous ROC
curves and include the ROC curve of the full model for comparison.
```{r}
full_fit <- glm(c("Outcome ~ ", paste(colnames(diabetes_dt[,1:8]), collapse = " + ") ), data=diabetes_dt, family = "binomial")
full_fit
```

```{r}
diabetes_dt[, full_prediction := predict(full_fit)]

melted <- melt(diabetes_dt, measure.vars = c("g_prediction", "b_prediction", "i_prediction", "full_prediction"), variable.name = "predictor", value.name = "response")

ggplot(melted, aes(d=as.numeric(Outcome), m=response, color=predictor)) +
  geom_roc() +
  geom_abline(intercept = 0, slope = 1)


```




